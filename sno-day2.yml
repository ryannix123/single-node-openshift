---
# =============================================================================
# Single Node OpenShift - Day 2 Operations Playbook
# Author: Ryan Nix <ryan.nix@gmail.com>
# Repo:   https://github.com/ryannix123/single-node-openshift
#
# Prerequisites:
#   ansible-galaxy collection install kubernetes.core community.general
#
# Usage:
#   ansible-playbook sno-day2.yml
#
# Overrides (use -e or -e @my-vars.yml):
#   storage_device   - Block device to wipe          (default: /dev/sdb)
#   storage_class    - LVM StorageClass name          (default: lvms-vg1)
#   lvm_channel      - Operator subscription channel  (default: stable-4.21)
#   registry_size    - Image registry PVC size        (default: 100Gi)
#   monitoring_size  - Prometheus PVC size            (default: 40Gi)
#
# See vars/defaults.yml for all variables.
# =============================================================================
- name: SNO Day 2 Operations
  hosts: localhost
  connection: local
  gather_facts: false

  vars_files:
    - "{{ playbook_dir }}/vars/defaults.yml"

  module_defaults:
    group/kubernetes.core.k8s:
      kubeconfig: "{{ kubeconfig }}"
    kubernetes.core.k8s_info:
      kubeconfig: "{{ kubeconfig }}"
    kubernetes.core.k8s_json_patch:
      kubeconfig: "{{ kubeconfig }}"

  tasks:

    # --------------------------------------------------------------------------
    # 0. Pre-flight checks
    # --------------------------------------------------------------------------
    - name: Verify oc / kubectl is available
      ansible.builtin.command: oc version --client
      register: oc_version
      changed_when: false
      failed_when: oc_version.rc != 0

    - name: Verify cluster is reachable
      kubernetes.core.k8s_info:
        kind: Node
      register: nodes
      failed_when: nodes.resources | length == 0

    # --------------------------------------------------------------------------
    # 0b. Discover node name and IP
    # --------------------------------------------------------------------------
    - name: Set sno_node fact
      ansible.builtin.set_fact:
        sno_node: "{{ nodes.resources[0].metadata.name }}"

    - name: Get SNO node internal IP
      ansible.builtin.set_fact:
        sno_node_ip: >-
          {{ nodes.resources[0].status.addresses |
             selectattr('type', 'equalto', 'InternalIP') |
             map(attribute='address') | first }}

    - name: Add SNO node to inventory for SSH tasks
      ansible.builtin.add_host:
        name: sno_node
        ansible_host: "{{ sno_node_ip }}"
        ansible_user: core
        ansible_ssh_private_key_file: "{{ ssh_key_path }}"
        ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"

    # --------------------------------------------------------------------------
    # 1. Wipe the secondary drive via SSH (skipped if LVM already present)
    # --------------------------------------------------------------------------
- name: Wipe secondary disk on SNO node
  hosts: sno_node
  gather_facts: false
  become: true

  vars_files:
    - "{{ playbook_dir }}/vars/defaults.yml"

  tasks:
    - name: Check if disk already has LVM children
      ansible.builtin.command: lsblk -J -o NAME,TYPE {{ storage_device }}
      register: lsblk_check
      changed_when: false
      failed_when: false

    - name: Set disk_has_lvm fact
      ansible.builtin.set_fact:
        disk_has_lvm: "{{ 'lvm' in lsblk_check.stdout }}"

    - name: Remove stale device mapper entries
      ansible.builtin.command: dmsetup remove_all -f
      changed_when: true
      failed_when: false

    - name: "Remove all partitions from kernel device table on {{ storage_device }}"
      ansible.builtin.command: partx -d {{ storage_device }}
      changed_when: true
      failed_when: false

    - name: "Wipe {{ storage_device }} - destroy partition table"
      ansible.builtin.command: sgdisk --zap-all {{ storage_device }}
      changed_when: true
      when: not disk_has_lvm

    - name: "Wipe {{ storage_device }} - zero first 10 MB"
      ansible.builtin.command: dd if=/dev/zero of={{ storage_device }} bs=1M count=10 oflag=direct
      changed_when: true
      when: not disk_has_lvm

    - name: "Force kernel to re-read partition table on {{ storage_device }}"
      ansible.builtin.command: blockdev --rereadpt {{ storage_device }}
      register: blockdev_result
      changed_when: blockdev_result.rc == 0
      failed_when: false
      when: not disk_has_lvm

    - name: Verify no partitions remain on {{ storage_device }}
      ansible.builtin.command: lsblk -J -o NAME,TYPE {{ storage_device }}
      register: lsblk_verify
      changed_when: false

    - name: Fail if partitions still present after wipe
      ansible.builtin.fail:
        msg: "Partitions still present on {{ storage_device }} after wipe: {{ lsblk_verify.stdout }}"
      when: '"part" in lsblk_verify.stdout'

    - name: Pause 5 seconds for kernel to settle
      ansible.builtin.pause:
        seconds: 5

- name: SNO Day 2 Operations (continued)
  hosts: localhost
  connection: local
  gather_facts: false

  vars_files:
    - "{{ playbook_dir }}/vars/defaults.yml"

  module_defaults:
    group/kubernetes.core.k8s:
      kubeconfig: "{{ kubeconfig }}"
    kubernetes.core.k8s_info:
      kubeconfig: "{{ kubeconfig }}"
    kubernetes.core.k8s_json_patch:
      kubeconfig: "{{ kubeconfig }}"

  tasks:


    # --------------------------------------------------------------------------
    # 2. Install the LVM Storage Operator
    # --------------------------------------------------------------------------
    - name: Create openshift-lvm-storage namespace
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: openshift-lvm-storage
            labels:
              openshift.io/cluster-monitoring: "true"

    - name: Create LVM Operator OperatorGroup
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: operators.coreos.com/v1
          kind: OperatorGroup
          metadata:
            name: openshift-lvm-storage-operatorgroup
            namespace: openshift-lvm-storage
          spec:
            targetNamespaces:
              - openshift-lvm-storage

    - name: Create LVM Operator Subscription
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: operators.coreos.com/v1alpha1
          kind: Subscription
          metadata:
            name: lvms-operator
            namespace: openshift-lvm-storage
          spec:
            channel: "{{ lvm_channel }}"
            installPlanApproval: Automatic
            name: lvms-operator
            source: redhat-operators
            sourceNamespace: openshift-marketplace

    - name: Wait for LVM InstallPlan to be created
      kubernetes.core.k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: InstallPlan
        namespace: openshift-lvm-storage
      register: lvm_installplan
      retries: 20
      delay: 10
      until: lvm_installplan.resources | length > 0

    - name: Approve any pending LVM InstallPlans
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: operators.coreos.com/v1alpha1
          kind: InstallPlan
          metadata:
            name: "{{ item.metadata.name }}"
            namespace: openshift-lvm-storage
          spec:
            approved: true
      loop: "{{ lvm_installplan.resources }}"
      when: not item.spec.approved

    - name: Show LVM subscription status (debug)
      kubernetes.core.k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: Subscription
        name: lvms-operator
        namespace: openshift-lvm-storage
      register: lvm_sub_debug

    - name: Print LVM subscription state
      ansible.builtin.debug:
        msg:
          - "currentCSV:   {{ lvm_sub_debug.resources[0].status.currentCSV | default('not set') }}"
          - "installedCSV: {{ lvm_sub_debug.resources[0].status.installedCSV | default('not set') }}"
          - "state:        {{ lvm_sub_debug.resources[0].status.state | default('not set') }}"

    - name: Wait for LVM Operator CSV to succeed
      kubernetes.core.k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-lvm-storage
      register: lvm_csv
      retries: "{{ operator_retries }}"
      delay: "{{ operator_delay }}"
      until: >
        lvm_csv.resources | length > 0 and
        lvm_csv.resources | map(attribute='status.phase') | select('equalto', 'Succeeded') | list | length > 0

    # --------------------------------------------------------------------------
    # 3. Create LVMCluster
    # --------------------------------------------------------------------------
    - name: Create LVMCluster
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: lvm.topolvm.io/v1alpha1
          kind: LVMCluster
          metadata:
            name: sno-lvmcluster
            namespace: openshift-lvm-storage
          spec:
            storage:
              deviceClasses:
                - name: vg1
                  thinPoolConfig:
                    name: thin-pool-1
                    sizePercent: 90
                    overprovisionRatio: 10
                  deviceSelector:
                    paths:
                      - "{{ storage_device }}"

    - name: Show raw LVMCluster status (debug)
      ansible.builtin.command: >
        oc get lvmcluster sno-lvmcluster -n openshift-lvm-storage -o jsonpath='{.status}'
      register: lvmcluster_raw
      changed_when: false

    - name: Print LVMCluster raw status
      ansible.builtin.debug:
        msg: "{{ lvmcluster_raw.stdout }}"

    - name: Wait for LVMCluster to become Ready
      ansible.builtin.command: >
        oc get lvmcluster sno-lvmcluster -n openshift-lvm-storage
        -o jsonpath='{.status.state}'
      register: lvm_ready
      retries: "{{ operator_retries }}"
      delay: "{{ operator_delay }}"
      until: lvm_ready.stdout == "Ready"
      changed_when: false

    # --------------------------------------------------------------------------
    # 4. Set lvms-vg1 as the default StorageClass
    # --------------------------------------------------------------------------
    - name: Get names of any currently-default StorageClasses
      ansible.builtin.command: >
        oc get sc -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class=="true")].metadata.name}'
      register: current_defaults
      changed_when: false

    - name: Remove default annotation from existing default StorageClasses
      ansible.builtin.command: >
        oc annotate sc {{ item }} storageclass.kubernetes.io/is-default-class=false --overwrite
      loop: "{{ current_defaults.stdout.split() | reject('equalto', storage_class) | list }}"
      when: current_defaults.stdout | length > 0

    - name: "Set {{ storage_class }} as the default StorageClass"
      ansible.builtin.command: >
        oc annotate sc {{ storage_class }} storageclass.kubernetes.io/is-default-class=true --overwrite
      register: sc_annotate
      changed_when: "'annotated' in sc_annotate.stdout"

    - name: Verify default StorageClass is set
      ansible.builtin.command: oc get sc
      register: sc_list
      changed_when: false

    - name: Print StorageClasses
      ansible.builtin.debug:
        msg: "{{ sc_list.stdout_lines }}"

    # --------------------------------------------------------------------------
    # 5. Patch the image registry to use persistent storage
    # --------------------------------------------------------------------------
    - name: Create PVC for image registry
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: image-registry-storage
            namespace: openshift-image-registry
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: "{{ registry_size }}"
            storageClassName: "{{ storage_class }}"

    - name: Patch image registry - enable managed state and PVC storage
      ansible.builtin.command: >
        oc patch configs.imageregistry.operator.openshift.io/cluster --type=merge
        -p '{"spec":{"managementState":"Managed","storage":{"pvc":{"claim":"image-registry-storage"}},"rolloutStrategy":"Recreate","replicas":1}}'
      changed_when: true

    - name: Wait for image registry to be Available
      ansible.builtin.command: >
        oc get configs.imageregistry.operator.openshift.io cluster
        -o jsonpath='{.status.conditions[?(@.type=="Available")].status}'
      register: registry_status
      retries: "{{ operator_retries }}"
      delay: "{{ operator_delay }}"
      until: registry_status.stdout == "True"
      changed_when: false

    # --------------------------------------------------------------------------
    # 6. Patch cluster monitoring to use persistent storage
    # --------------------------------------------------------------------------
    - name: Create or update cluster-monitoring-config ConfigMap
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: cluster-monitoring-config
            namespace: openshift-monitoring
          data:
            config.yaml: |
              prometheusK8s:
                retention: {{ monitoring_retention }}
                volumeClaimTemplate:
                  metadata:
                    name: prometheus-k8s-data
                  spec:
                    storageClassName: {{ storage_class }}
                    resources:
                      requests:
                        storage: {{ monitoring_size }}
              alertmanagerMain:
                volumeClaimTemplate:
                  metadata:
                    name: alertmanager-data
                  spec:
                    storageClassName: {{ storage_class }}
                    resources:
                      requests:
                        storage: {{ alertmanager_size }}

    - name: Wait for Prometheus pods to be Running
      ansible.builtin.command: >
        oc get pods -n openshift-monitoring -l app.kubernetes.io/name=prometheus
        --field-selector=status.phase=Running --no-headers
      register: prom_pods
      retries: "{{ operator_retries }}"
      delay: "{{ operator_delay }}"
      until: prom_pods.stdout_lines | length > 0
      changed_when: false

    # --------------------------------------------------------------------------
    # 7. Install OpenShift Virtualization
    # --------------------------------------------------------------------------
    - name: Create openshift-cnv namespace
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: openshift-cnv
            labels:
              openshift.io/cluster-monitoring: "true"

    - name: Create CNV OperatorGroup
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: operators.coreos.com/v1
          kind: OperatorGroup
          metadata:
            name: kubevirt-hyperconverged-group
            namespace: openshift-cnv
          spec:
            targetNamespaces:
              - openshift-cnv

    - name: Create CNV Subscription
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: operators.coreos.com/v1alpha1
          kind: Subscription
          metadata:
            name: hco-operatorhub
            namespace: openshift-cnv
          spec:
            channel: stable
            installPlanApproval: Automatic
            name: kubevirt-hyperconverged
            source: redhat-operators
            sourceNamespace: openshift-marketplace

    - name: Fetch CSVs in openshift-cnv (debug)
      kubernetes.core.k8s_info:
        api_version: operators.coreos.com/v1alpha1
        kind: ClusterServiceVersion
        namespace: openshift-cnv
      register: cnv_csv_debug

    - name: Wait for CNV CSV to succeed
      ansible.builtin.command: >
        oc get csv -n openshift-cnv
        -o jsonpath='{.items[?(@.status.phase=="Succeeded")].metadata.name}'
      register: cnv_csv
      retries: "{{ operator_retries }}"
      delay: "{{ operator_delay }}"
      until: cnv_csv.stdout | length > 0
      changed_when: false

    - name: Create HyperConverged instance
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: hco.kubevirt.io/v1beta1
          kind: HyperConverged
          metadata:
            name: kubevirt-hyperconverged
            namespace: openshift-cnv
          spec:
            enableCommonBootImageImport: true

    - name: Wait for HyperConverged to be Available
      ansible.builtin.command: >
        oc get hyperconverged kubevirt-hyperconverged -n openshift-cnv
        -o jsonpath='{.status.conditions[?(@.type=="Available")].status}'
      register: hco_status
      retries: "{{ operator_retries }}"
      delay: "{{ operator_delay }}"
      until: hco_status.stdout == "True"
      changed_when: false

    # --------------------------------------------------------------------------
    # Summary
    # --------------------------------------------------------------------------
    - name: Print completion summary
      ansible.builtin.debug:
        msg:
          - "✅  Drive {{ storage_device }} wiped"
          - "✅  LVM Storage Operator installed ({{ lvm_channel }})"
          - "✅  LVMCluster created and Ready"
          - "✅  StorageClass {{ storage_class }} set as cluster default"
          - "✅  Image registry patched → {{ registry_size }} PVC"
          - "✅  Cluster monitoring patched → {{ monitoring_size }} PVC ({{ monitoring_retention }} retention)"
          - "✅  OpenShift Virtualization installed and HyperConverged is Available"